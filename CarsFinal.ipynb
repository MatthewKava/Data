{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and data 'CarPrice_Assignment.csv' from Kaggle\n",
    "### https://www.kaggle.com/datasets/hellbuoy/car-price-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import f,norm,sem\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.weightstats as sms\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CarPrice_Assignment.csv')\n",
    "#Set view columns to max to avoid truncated data\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "dictionary = pd.read_excel('Data Dictionary - carprices.xlsx')\n",
    "dictionary = dictionary[['Unnamed: 7','Unnamed: 11']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dictionary.dropna()\n",
    "col_names = ['Variable', 'Description']\n",
    "dictionary.columns = col_names\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Data and Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new df for cleaned data, keep a copy of the original df if needed later\n",
    "dfc = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Car_ID is unique for each car so it is dropped\n",
    "dfc = dfc.drop('car_ID',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfc.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Symboling is not continous data but categorical, see data dictionary, change to object.\n",
    "dfc['symboling'] = dfc['symboling'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Car Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfc['CarName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is few cars of the same model so I will replace the full car name with just the brand\n",
    "\n",
    "#First make an array of all car brands in column\n",
    "\n",
    "brands = ['alfa-romero','audi','bmw','chevrolet','dodge','honda','isuzu','jaguar','mazda','buick',\n",
    "          'mercury','mitsubishi','nissan','peugeot','plymouth','porsche','renault','saab','subaru',\n",
    "          'toyota','volkswagen','volvo']\n",
    "list1 = df.CarName.unique()\n",
    "\n",
    "#Use a loop to search for strings that contain a brand name and replace string with brand name, i.e. removing model name.\n",
    "\n",
    "for i in range(len(brands)):\n",
    "    brand = brands[i]\n",
    "    l1 = [k for k in list1 if brand in k]\n",
    "    dfc = dfc.replace(l1,brand)\n",
    "    \n",
    "#Replacing misspelled names manually\n",
    "\n",
    "dfc = dfc.replace(['Nissan versa'], 'nissan')\n",
    "dfc = dfc.replace(['vokswagen rabbit'], 'volkswagen')\n",
    "dfc = dfc.replace(['porcshce panamera'], 'porsche')\n",
    "dfc = dfc.replace([ 'maxda rx3', 'maxda glc deluxe'],'mazda')\n",
    "dfc = dfc.replace(['toyouta tercel'],'toyota')\n",
    "dfc = dfc.replace(['vw dasher','vw rabbit'],'volkswagen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfc['CarName'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking other categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['enginelocation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['symboling'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['doornumber'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['enginetype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['aspiration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['cylindernumber'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The vast majority of cars are front engine, this will not help in regression so is dropped\n",
    "dfc = dfc.drop('enginelocation',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a copy of the cleaned data.\n",
    "df_cleaned = dfc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.round(df.corr(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get all numeric data columns and use boxpplots to check for skewed data.\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "numericdf = df.select_dtypes(include=numerics)\n",
    "\n",
    "for i in range(len(numericdf.columns)):\n",
    "    plt.boxplot(numericdf[numericdf.columns[i]])\n",
    "    plt.title(numericdf.columns[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price looks skewed, may use the log 10 function in MLR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a difference in mpg based on fuel types ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new column for average mpg, take the mean average of city and highway mpg combined.\n",
    "dfc['averagempg'] = (dfc['citympg'] + dfc['highwaympg'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualise the data on a boxplot\n",
    "\n",
    "#Split the data into gas and diesel\n",
    "gasmpg = dfc[dfc['fueltype']=='gas']['averagempg']\n",
    "dieselmpg = dfc[dfc['fueltype']=='diesel']['averagempg']\n",
    "\n",
    "#Plotting on boxplot\n",
    "plt.boxplot([gasmpg.values,dieselmpg.values])\n",
    "plt.xticks([1,2],['gas','diesel'])\n",
    "plt.ylabel('Average Miles per Gallon')\n",
    "plt.title('Mpg by Fueltype')\n",
    "plt.show()\n",
    "\n",
    "print('Gas mean average mpg: ',gasmpg.mean(), '\\nfrom sample size: ', len(gasmpg))\n",
    "print('\\nDiesel mean average mpg: ',dieselmpg.mean(), '\\nfrom sample size: ', len(dieselmpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['averagempg'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[30,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fisher's F test\n",
    "var_gas = np.var(gasmpg,ddof=1)\n",
    "var_diesel = np.var(dieselmpg,ddof=1)\n",
    "print(var_gas)\n",
    "print(var_diesel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ratio = var_diesel/var_gas\n",
    "print(f_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get p value for difference in variances\n",
    "dfn = len(dieselmpg)-1\n",
    "dfd = len(gasmpg)-1\n",
    "alpha = 0.05\n",
    "p_one_tailed = f.sf(f_ratio, dfn, dfd)\n",
    "p = p_one_tailed * 2\n",
    "print('pvalue is: ',p)\n",
    "if p < alpha:\n",
    "    print('Reject the null hypothesis that variances of the samples are equal')\n",
    "else:\n",
    "    print('Null hypothesis cannot be rejected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a pooled standard deviation to perform t-test\n",
    "stats.ttest_ind(a = gasmpg, b = dieselmpg, equal_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sms.CompareMeans(sms.DescrStatsW(gasmpg), sms.DescrStatsW(dieselmpg))\n",
    "print(cm.tconfint_diff(usevar='pooled'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At 0.05 significance diesel cars get between 2.01 and 8.05 more mpg than gas. However, a gas Honda Civic has the best average\n",
    "#### mpg in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise distribution of drivewheels by carbody type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define a funciton to print the value counts of drivewheels by carbody type\n",
    "def get(name):\n",
    "    a = dfc[dfc['carbody']==name]['drivewheel']\n",
    "    print(name,' have the following drivewheels\\n',a.value_counts(), '\\n \\n')\n",
    "\n",
    "#Use a loop to put each carbody type through the above function\n",
    "\n",
    "for i in range(len(dfc['carbody'].unique())):\n",
    "    get(dfc['carbody'].unique()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Organise results into categories and frequency\n",
    "body = (\"Convertible\", \"Hatchback\", \"Sedan\",\"Wagon\",\"Hardtop\")\n",
    "count = {\n",
    "    'front wheel drive': (1,49,57,12,1),\n",
    "    'rear wheel drive': (5,19,36,9,7),\n",
    "    '4 wheel drive': (0,2,3,4,0),\n",
    "}\n",
    "\n",
    "x = np.arange(len(body))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "#Plot grouped bar chart\n",
    "fig, ax = plt.subplots(figsize =(8,8))\n",
    "\n",
    "for attribute, measurement in count.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    multiplier += 1\n",
    "\n",
    "\n",
    "\n",
    "ax.set_ylabel('Number of cars')\n",
    "ax.set_title('Drivewheel by bodytype')\n",
    "ax.set_xticks(x + width, body)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylim(0, 65)\n",
    "ax.set_xticklabels([\"0\",\"Convertible\", \"Hatchback\", \"Sedan\",\"Wagon\",\"Hardtop\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hatchbacks and Sedans are predominantely front wheel drive. They are also the most common car body type in the sample.\n",
    "#### Convertibles and Hardtops are predominantely rear wheel drive, although the samples are small (n < 10)\n",
    "#### Wagons have a mix of all 3 drivewheels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the best and worst performing cars in the data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Horsepower is a good metric to use to determine performance.\n",
    "#However, power to weight ratio is a better metric for sports car perfomance as a car that has a high\n",
    "#horsepower but is heavy will be slow.\n",
    "\n",
    "#Make new column with power to weight ratio (horsepower / curbweight) multiplied by 2000 to convert lbs to US ton.\n",
    "#Unit horsepower per US ton.\n",
    "\n",
    "\n",
    "dfc['bhp/weight'] = (dfc['horsepower']/dfc['curbweight'])*2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the largest values in new column created\n",
    "dfc['bhp/weight'].nlargest(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a list of the index of the cars with best power to weight ratio\n",
    "best = dfc['bhp/weight'].nlargest(n=5).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best performing cars\\n')\n",
    "#Use the original dataframe to get full name of the top 5 performing cars\n",
    "for i in range(5):\n",
    "    print(i+1, df['CarName'].iloc[best[i]], 'has a power to weight ratio of:' , round(dfc['bhp/weight'].iloc[best[i]],1), 'bhp per US ton. Price : ', df['price'].iloc[best[i]], ' USD\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Worst performing cars\\n')\n",
    "#Repeat steps above for the worst cars\n",
    "worst = dfc['bhp/weight'].nsmallest(n=5).index.values\n",
    "for i in range(5):\n",
    "    print(i+1, df['CarName'].iloc[worst[i]], 'has a power to weight ratio of:' , round(dfc['bhp/weight'].iloc[worst[i]],1), 'bhp per US ton. Price : ', df['price'].iloc[worst[i]], ' USD\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which brands have the best insurance ratings ? ('symboling', +3 is risky, -3 is very safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create blank DataFrame for normalised symbol counts\n",
    "dfs = pd.DataFrame({'Blank': [0]}, index =['-2','-1','0','1','2','3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a loop that adds a new column for each brand containing the normalised value counts\n",
    "for i in range(len(brands)):\n",
    "    symb = dfc[dfc['CarName']== brands[i]]['symboling']\n",
    "    dfs[brands[i]] = symb.value_counts(normalize=True, dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop blank\n",
    "dfs1 = dfs.drop('Blank',axis=1)\n",
    "#Transpose data frame and plot with color gradient. Red is risky, blue is safe.\n",
    "dfs1 = dfs1.T\n",
    "#Sort data by which brand has the highest percetage of good symbols.\n",
    "dfs1 = dfs1.sort_values(by=['-2','-1','0','1'],ascending=False)\n",
    "\n",
    "#Convert normalise to percentage\n",
    "dfs1 = dfs1 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs1.plot(\n",
    "    kind = 'bar',\n",
    "    stacked = True,\n",
    "    title = 'Insurance symbols by car brand',\n",
    "    mark_right = True,\n",
    "    colormap='bwr',\n",
    "    rot=65,\n",
    "    figsize = (10,10),\n",
    "    ylabel= 'Percentage of cars in sample',\n",
    "    xlabel= 'Brand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volvo has the best insurance ratings while Saab has the worst in our given sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do different drivewheels tend to have a difference in horsepower ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcyl = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "rwd = dfcyl[dfcyl['drivewheel']=='rwd']['horsepower']\n",
    "fwd = dfcyl[dfcyl['drivewheel']=='fwd']['horsepower']\n",
    "fourwd = dfcyl[dfcyl['drivewheel']=='4wd']['horsepower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting on boxplot\n",
    "plt.boxplot([rwd.values,fwd.values,fourwd.values])\n",
    "plt.xticks([1,2,3],['rear wheel drive','front wheel drive','four wheel drive'])\n",
    "plt.ylabel('horsepower')\n",
    "plt.title('horsepower by drivewheel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rear wheel drive may have slightly more horsepower. Use an ANOVA test to find if the means of the grouped data are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_oneway(rwd, fwd, fourwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is statistical difference between the means of at least two of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform Tukey post-hoc test.\n",
    "print(pairwise_tukeyhsd(endog=dfcyl['horsepower'],\n",
    "                        groups=dfcyl['drivewheel'],\n",
    "                        alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rwd has 38.4 and 47.7 hp more than 4wd and fwd respectively at 0.05 significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make MLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include all numeric and categorical variables for MLR model.\n",
    "#New df for the model, dfm.\n",
    "\n",
    "dfm = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.get_dummies(dfm, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use scatter graphs to find relationship between variables and log10 of the price.\n",
    "\n",
    "dfm = pd.get_dummies(dfm, drop_first = True)\n",
    "\n",
    "for i in range(13):\n",
    "    plt.scatter(dfm[dfm.columns[i]], np.log10(dfm['price']))\n",
    "    plt.title(dfm.columns[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like a strong correlation between carwidth, curbweight, horsepower, enginesize and city mpg with log 10 of the price.\n",
    "#Make a model with these variables\n",
    "X = dfm[['carwidth','curbweight','horsepower','enginesize','citympg']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "dfm['log10 price'] = np.log10(dfm['price'])\n",
    "y = dfm['log10 price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to make a MLR model an print a summary of results. \n",
    "\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "def get_stats():\n",
    "    regr.fit(X, y)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    predictions = model.predict(X) \n",
    "\n",
    "    print_model = model.summary()\n",
    "    print(print_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove variable with highest p value, re-run model and repeat until all variables have p less than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfm[['carwidth','curbweight','horsepower','enginesize']]\n",
    "X = sm.add_constant(X)\n",
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfm[['carwidth','curbweight','horsepower']]\n",
    "X = sm.add_constant(X)\n",
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop car width to reduce multicollinearity\n",
    "X = X.drop('carwidth', axis = 1)\n",
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 8))\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "fig = plt.figure(figsize = (12, 8))\n",
    "fig = sm.graphics.plot_regress_exog(model, 'curbweight', fig = fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 8))\n",
    "fig = sm.graphics.plot_regress_exog(model, 'horsepower', fig = fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horsepower is right skewed, try log10 for horsepower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['log10 horsepower'] = np.log10(X['horsepower'])\n",
    "X = X.drop('horsepower', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 8))\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "fig = sm.graphics.plot_regress_exog(model, 'log10 horsepower', fig = fig)\n",
    "plt.title('Regrssion Plots for log10 Horsepower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model\n",
    "# Get different Variables for diagnostic\n",
    "residuals = results.resid\n",
    "fitted_value = results.fittedvalues\n",
    "stand_resids = results.resid_pearson\n",
    "influence = results.get_influence()\n",
    "leverage = influence.hat_matrix_diag\n",
    "  \n",
    "# PLot different diagnostic plots\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "  \n",
    "plt.style.use('seaborn')\n",
    "  \n",
    "# Residual vs Fitted Plot\n",
    "sns.scatterplot(x=fitted_value, y=residuals, ax=ax[0, 0])\n",
    "ax[0, 0].axhline(y=0, color='grey', linestyle='dashed')\n",
    "ax[0, 0].set_xlabel('Fitted Values')\n",
    "ax[0, 0].set_ylabel('Residuals')\n",
    "ax[0, 0].set_title('Residuals vs Fitted Fitted')\n",
    "  \n",
    "# Normal Q-Q plot\n",
    "sm.qqplot(residuals, fit=True, line='45',ax=ax[0, 1], c='#4C72B0')\n",
    "ax[0, 1].set_title('Normal Q-Q')\n",
    "  \n",
    "# Scale-Location Plot\n",
    "sns.scatterplot(x=fitted_value, y=residuals, ax=ax[1, 0])\n",
    "ax[1, 0].axhline(y=0, color='grey', linestyle='dashed')\n",
    "ax[1, 0].set_xlabel('Fitted values')\n",
    "ax[1, 0].set_ylabel('Sqrt(standardized residuals)')\n",
    "ax[1, 0].set_title('Scale-Location Plot')\n",
    "  \n",
    "# Residual vs Leverage Plot\n",
    "sns.scatterplot(x=leverage, y=stand_resids, ax=ax[1, 1])\n",
    "ax[1, 1].axhline(y=0, color='grey', linestyle='dashed')\n",
    "ax[1, 1].set_xlabel('Leverage')\n",
    "ax[1, 1].set_ylabel('Sqrt(standardized residuals)')\n",
    "ax[1, 1].set_title('Residuals vs Leverage Plot')\n",
    "  \n",
    "  \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "  \n",
    "# PLot Cook's distance plot\n",
    "sm.graphics.influence_plot(results, criterion=\"cooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[126:129,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfm[['curbweight','horsepower',\n",
    "       'symboling_-2', 'symboling_0', 'symboling_1', 'symboling_2',\n",
    "       'symboling_3', 'CarName_audi', 'CarName_bmw',\n",
    "       'CarName_buick', 'CarName_chevrolet', 'CarName_dodge', 'CarName_honda',\n",
    "       'CarName_isuzu', 'CarName_jaguar', 'CarName_mazda', 'CarName_mercury',\n",
    "       'CarName_mitsubishi', 'CarName_nissan', 'CarName_peugeot',\n",
    "       'CarName_plymouth', 'CarName_porsche', 'CarName_renault',\n",
    "       'CarName_saab', 'CarName_subaru', 'CarName_toyota',\n",
    "       'CarName_volkswagen', 'CarName_volvo',\n",
    "       'fueltype_gas', 'aspiration_turbo',\n",
    "       'doornumber_two', 'carbody_hardtop',\n",
    "       'carbody_hatchback', 'carbody_sedan', 'carbody_wagon',\n",
    "       'drivewheel_fwd', 'drivewheel_rwd',\n",
    "       'enginetype_dohcv', 'enginetype_l', 'enginetype_ohc', 'enginetype_ohcf',\n",
    "       'enginetype_ohcv', 'enginetype_rotor',\n",
    "       'cylindernumber_five', 'cylindernumber_four', 'cylindernumber_six',\n",
    "       'cylindernumber_three', 'cylindernumber_twelve', 'cylindernumber_two',\n",
    "         'fuelsystem_2bbl', 'fuelsystem_4bbl',\n",
    "       'fuelsystem_idi', 'fuelsystem_mfi', 'fuelsystem_mpfi',\n",
    "       'fuelsystem_spdi', 'fuelsystem_spfi']]\n",
    "X = sm.add_constant(X)\n",
    "X['log10 horsepower'] = np.log10(X['horsepower'])\n",
    "X = X.drop('horsepower', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfm[['curbweight','horsepower','symboling_-2','CarName_bmw','CarName_mitsubishi','CarName_peugeot',\n",
    "         'CarName_plymouth','CarName_subaru','CarName_toyota','fueltype_gas','carbody_hatchback',\n",
    "         'carbody_wagon','enginetype_ohcf','cylindernumber_five','fuelsystem_idi','fuelsystem_mpfi']]\n",
    "X = sm.add_constant(X)\n",
    "X['log10 horsepower'] = np.log10(X['horsepower'])\n",
    "X = X.drop('horsepower', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfm[['curbweight','horsepower','symboling_-2','CarName_bmw','CarName_mitsubishi','CarName_peugeot',\n",
    "         'CarName_plymouth','CarName_subaru','CarName_toyota','fueltype_gas','carbody_hatchback',\n",
    "         'carbody_wagon','enginetype_ohcf','fuelsystem_idi','fuelsystem_mpfi']]\n",
    "X = sm.add_constant(X)\n",
    "X['log10 horsepower'] = np.log10(X['horsepower'])\n",
    "X = X.drop('horsepower', axis = 1)\n",
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('symboling_-2',axis = 1)\n",
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('carbody_hatchback',axis = 1)\n",
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('CarName_plymouth',axis = 1)\n",
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "XMLR = X\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "results = model\n",
    "# Get different Variables for diagnostic\n",
    "residuals = results.resid\n",
    "fitted_value = results.fittedvalues\n",
    "stand_resids = results.resid_pearson\n",
    "influence = results.get_influence()\n",
    "leverage = influence.hat_matrix_diag\n",
    "  \n",
    "# PLot different diagnostic plots\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "  \n",
    "plt.style.use('seaborn')\n",
    "  \n",
    "# Residual vs Fitted Plot\n",
    "sns.scatterplot(x=fitted_value, y=residuals, ax=ax[0, 0])\n",
    "ax[0, 0].axhline(y=0, color='grey', linestyle='dashed')\n",
    "ax[0, 0].set_xlabel('Fitted Values')\n",
    "ax[0, 0].set_ylabel('Residuals')\n",
    "ax[0, 0].set_title('Residuals vs Fitted Fitted')\n",
    "  \n",
    "# Normal Q-Q plot\n",
    "sm.qqplot(residuals, fit=True, line='45',ax=ax[0, 1], c='#4C72B0')\n",
    "ax[0, 1].set_title('Normal Q-Q')\n",
    "  \n",
    "# Scale-Location Plot\n",
    "sns.scatterplot(x=fitted_value, y=residuals, ax=ax[1, 0])\n",
    "ax[1, 0].axhline(y=0, color='grey', linestyle='dashed')\n",
    "ax[1, 0].set_xlabel('Fitted values')\n",
    "ax[1, 0].set_ylabel('Sqrt(standardized residuals)')\n",
    "ax[1, 0].set_title('Scale-Location Plot')\n",
    "  \n",
    "# Residual vs Leverage Plot\n",
    "sns.scatterplot(x=leverage, y=stand_resids, ax=ax[1, 1])\n",
    "ax[1, 1].axhline(y=0, color='grey', linestyle='dashed')\n",
    "ax[1, 1].set_xlabel('Leverage')\n",
    "ax[1, 1].set_ylabel('Sqrt(standardized residuals)')\n",
    "ax[1, 1].set_title('Residuals vs Leverage Plot')\n",
    "  \n",
    "  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is our best model, adjusted R-squared of 0.924 and an AIC of -557.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create test and train datasets to find the accuracy of the above MLR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split data.\n",
    "\n",
    "X = XMLR\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 30)\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "start1 = timer()\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "end1 = timer()\n",
    "\n",
    "MLR = round(metrics.r2_score(y_test, y_pred), 2)\n",
    "print(regr)\n",
    "print('RMSE = ', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),4))\n",
    "print(\"R-squared score =\", round(metrics.r2_score(y_test, y_pred), 2))\n",
    "print(\"Explain variance score =\", round(metrics.explained_variance_score(y_test, y_pred), 2))\n",
    "\n",
    "#Use k-fold cross validation to assess the model\n",
    "    \n",
    "\n",
    "scores = cross_val_score(regr, X = X_train, y = y_train, cv = 10, n_jobs = 1)\n",
    "print('K-fold cross validation score accuracy: ', np.round(np.mean(scores),3), '+/-', np.round(np.std(scores),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regrssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include all variables in the first random forest regression.\n",
    "regr = RandomForestRegressor(n_estimators=100)\n",
    "X = dfm.drop(['price','log10 price'],axis=1)\n",
    "y = dfm['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 30)\n",
    "\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "\n",
    "RFR = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),4)\n",
    "print(regr)\n",
    "print('RMSE = ', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),4))\n",
    "print(\"R-squared score =\", round(metrics.r2_score(y_test, y_pred), 2))\n",
    "print(\"Explain variance score =\", round(metrics.explained_variance_score(y_test, y_pred), 2))\n",
    "\n",
    "#Use k-fold cross validation to assess the model\n",
    "    \n",
    "\n",
    "scores = cross_val_score(regr, X = X_train, y = y_train, cv = 10, n_jobs = 1)\n",
    "print('K-fold cross validation score accuracy: ', np.round(np.mean(scores),3), '+/-', np.round(np.std(scores),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Find importance of features.\n",
    "feature_imp = pd.Series(regr.feature_importances_,X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "sorted_idx = regr.feature_importances_.argsort()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.figure(figsize=(12,12),dpi=80)\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.barh(X_test.columns[sorted_idx], regr.feature_importances_[sorted_idx])\n",
    "plt.title('Importance in Random Forest Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following variables give the best accuracy \n",
    "X = dfm[['curbweight','enginesize','highwaympg','horsepower','carwidth','citympg','wheelbase']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 30)\n",
    "\n",
    "start2 = timer()\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "end2 = timer()\n",
    "\n",
    "RFR = round(metrics.r2_score(y_test, y_pred), 2)\n",
    "print(regr)\n",
    "print('RMSE = ', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),4))\n",
    "print(\"R-squared score =\", round(metrics.r2_score(y_test, y_pred), 2))\n",
    "print(\"Explain variance score =\", round(metrics.explained_variance_score(y_test, y_pred), 2))\n",
    "\n",
    "#Use k-fold cross validation to assess the model\n",
    "    \n",
    "\n",
    "scores = cross_val_score(regr, X = X_train, y = y_train, cv = 10, n_jobs = 1)\n",
    "print('K-fold cross validation score accuracy: ', np.round(np.mean(scores),3), '+/-', np.round(np.std(scores),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA with MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = pd.get_dummies(dfp, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfp.drop('price',axis = 1)\n",
    "y = np.log10(dfp['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_scaled = StandardScaler().fit_transform(X)\n",
    "pca=PCA(n_components=30)\n",
    "X_red = pca.fit_transform(dfp_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before PCA: ', dfp_scaled.shape)\n",
    "print('Shape after PCA: ', X_red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the components with a value above 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals = 4)*100)[0:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 79.8 % is explained by 22 components, reduced from 67 originally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_red_train = pca.fit_transform(StandardScaler().fit_transform(X_train))\n",
    "X_red_test = pca.transform(StandardScaler().fit_transform(X_test))[:,0:21]\n",
    "lm = LinearRegression()\n",
    "\n",
    "start3 = timer()\n",
    "\n",
    "pcr = lm.fit(X_red_train[:,0:21], y_train)\n",
    "y_pred = pcr.predict(X_red_test)\n",
    "\n",
    "end3 = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC = round(metrics.r2_score(y_test, y_pred), 2)\n",
    "print('RMSE: ', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),4))\n",
    "print(\"R-squared score =\", round(metrics.r2_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate accuracy and efficiency of all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Multiple Linear Regression R squared: ', MLR, \n",
    "      '\\nRandom Forest Regression R squared: ', RFR, \n",
    "      '\\nPrincipal Component Analysis R squared:', PC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time for Multiple Linear Regression: ', np.round(end1 - start1, 4), 's')\n",
    "print('Time for Random Forest: ', np.round(end2 - start2, 4), 's')\n",
    "print('Time for Principal Component Analysis: ', np.round(end3 - start3, 4), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Method': ['Multiple Linear Regression','Random Forest Regression','Principal Component Analysis'],\n",
    "        'R Squared score on test data': [MLR, RFR, PC],\n",
    "        'Time (s)': [np.round(end1 - start1, 4), np.round(end2 - start2, 4), np.round(end3 - start3, 4)]}\n",
    "df_final = pd.DataFrame(data=data)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All three methods have similar accuracies after being tested on the same test data.\n",
    "### Each method provides a good prediction for the price of a car.\n",
    "### Random Forest is the most accurate but slow.\n",
    "### PCA is a little less accurate but is 100 fold faster than Random Forest which may be important for a larger sample.\n",
    "### MLR has good accuracy in a reasonable time. It is the best model to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
